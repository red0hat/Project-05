{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Identify Salient Features Using $\\ell1$-penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/assets/identify_features.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain and Data\n",
    "\n",
    "This is the second step in exploring feature selection on a dataset with many features, most of which are not relevant.  The dataset is the synthetic madelon data set from the previous step.  A simple logistic regresison on all features was not effective.  \n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "From step 1, the logisitic regression is equivalant to guessing.  This step is to start improve on the logistic regression by dropping irrelevant features.  \n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "This is exploring the solution space.  LASSO is a good place to start to see if automated feature selection is feasible.\n",
    "\n",
    "### Metric\n",
    "\n",
    "The metric from step 1 will be reused.  It is the mean accuracy of the prediction.  At least 2 test/train splits will be done.  First with the same random state as the previous step then with a new random state.  \n",
    "\n",
    "### Benchmark \n",
    "\n",
    "The previous step returned a mean accuracy of 52.0% for the test dataset.  If LASSO is an effective feature selection model, a marked improvement of the metric shoudl be seen.  Also, a smaller spread of the training score to test score is expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation:\n",
    "A similar pipeline to the previous step will be utilized.  The logistic regression model will include the parameter penalty='l1'.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import chdir, getcwd;\n",
    "chdir('../')\n",
    "from  lib.project_5 import load_data_from_database, make_data_dict, general_transformer, general_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_490</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>482</td>\n",
       "      <td>470</td>\n",
       "      <td>480</td>\n",
       "      <td>479</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>436</td>\n",
       "      <td>475</td>\n",
       "      <td>481</td>\n",
       "      <td>470</td>\n",
       "      <td>...</td>\n",
       "      <td>463</td>\n",
       "      <td>471</td>\n",
       "      <td>496</td>\n",
       "      <td>763</td>\n",
       "      <td>478</td>\n",
       "      <td>507</td>\n",
       "      <td>483</td>\n",
       "      <td>488</td>\n",
       "      <td>435</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>479</td>\n",
       "      <td>529</td>\n",
       "      <td>499</td>\n",
       "      <td>495</td>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "      <td>454</td>\n",
       "      <td>478</td>\n",
       "      <td>486</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>497</td>\n",
       "      <td>474</td>\n",
       "      <td>483</td>\n",
       "      <td>420</td>\n",
       "      <td>480</td>\n",
       "      <td>452</td>\n",
       "      <td>486</td>\n",
       "      <td>475</td>\n",
       "      <td>481</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>541</td>\n",
       "      <td>484</td>\n",
       "      <td>513</td>\n",
       "      <td>484</td>\n",
       "      <td>442</td>\n",
       "      <td>476</td>\n",
       "      <td>459</td>\n",
       "      <td>477</td>\n",
       "      <td>...</td>\n",
       "      <td>450</td>\n",
       "      <td>482</td>\n",
       "      <td>501</td>\n",
       "      <td>542</td>\n",
       "      <td>497</td>\n",
       "      <td>518</td>\n",
       "      <td>486</td>\n",
       "      <td>476</td>\n",
       "      <td>522</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>486</td>\n",
       "      <td>501</td>\n",
       "      <td>554</td>\n",
       "      <td>501</td>\n",
       "      <td>514</td>\n",
       "      <td>482</td>\n",
       "      <td>534</td>\n",
       "      <td>477</td>\n",
       "      <td>494</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>455</td>\n",
       "      <td>483</td>\n",
       "      <td>478</td>\n",
       "      <td>397</td>\n",
       "      <td>502</td>\n",
       "      <td>482</td>\n",
       "      <td>480</td>\n",
       "      <td>465</td>\n",
       "      <td>539</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>482</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>465</td>\n",
       "      <td>557</td>\n",
       "      <td>488</td>\n",
       "      <td>397</td>\n",
       "      <td>479</td>\n",
       "      <td>508</td>\n",
       "      <td>481</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>480</td>\n",
       "      <td>503</td>\n",
       "      <td>589</td>\n",
       "      <td>485</td>\n",
       "      <td>497</td>\n",
       "      <td>481</td>\n",
       "      <td>468</td>\n",
       "      <td>451</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat_000  feat_001  feat_002  feat_003  feat_004  feat_005  feat_006  \\\n",
       "1679       482       470       480       479       494       479       436   \n",
       "1445       479       529       499       495       481       481       454   \n",
       "352        485       477       541       484       513       484       442   \n",
       "552        486       501       554       501       514       482       534   \n",
       "338        482       476       476       465       557       488       397   \n",
       "\n",
       "      feat_007  feat_008  feat_009    ...     feat_490  feat_491  feat_492  \\\n",
       "1679       475       481       470    ...          463       471       496   \n",
       "1445       478       486       474    ...          497       474       483   \n",
       "352        476       459       477    ...          450       482       501   \n",
       "552        477       494       473    ...          455       483       478   \n",
       "338        479       508       481    ...          479       480       503   \n",
       "\n",
       "      feat_493  feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  \n",
       "1679       763       478       507       483       488       435       403  \n",
       "1445       420       480       452       486       475       481       506  \n",
       "352        542       497       518       486       476       522       476  \n",
       "552        397       502       482       480       465       539       499  \n",
       "338        589       485       497       481       468       451       461  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'user_name' : \"dsi_student\", \n",
    "          'password' : \"correct horse battery staple\",\n",
    "          'url': 'joshuacook.me',\n",
    "          'port' : \"5432\", \n",
    "          'database' : \"dsi\", \n",
    "          'table' : \"madelon\"}\n",
    "\n",
    "madelon_df = load_data_from_database(**params)\n",
    "madelon_df.drop('index', axis =1, inplace=True)\n",
    "\n",
    "y = madelon_df['label']\n",
    "X = madelon_df.drop('label', axis =1)\n",
    "\n",
    "baseline = make_data_dict(X,y,random_state=43)\n",
    "baseline[0]['X_train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "X_train = baseline[-1]['X_train']\n",
    "y_train = baseline[-1]['y_train']\n",
    "X_test = baseline[-1]['X_test']\n",
    "y_test = baseline[-1]['y_test']\n",
    "\n",
    "scale = StandardScaler()\n",
    "baseline.append(general_transformer(scale, X_train, y_train, X_test, y_test))\n",
    "\n",
    "X_train = baseline[-1]['X_train']\n",
    "y_train = baseline[-1]['y_train']\n",
    "X_test = baseline[-1]['X_test']\n",
    "y_test = baseline[-1]['y_test']\n",
    "\n",
    "LogReg =LogisticRegression(n_jobs=-1,verbose =2, penalty='l1')\n",
    "baseline.append(general_model(LogReg,X_train, y_train, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The mean accuracy of the training set is 79.07%.\n",
      "The mean accuracy of the test set is 50.80%.\n"
     ]
    }
   ],
   "source": [
    "print \"\\n\"\n",
    "print \"The mean accuracy of the training set is {:.2f}%.\".format (baseline[-1]['train_score']*100)\n",
    "print \"The mean accuracy of the test set is {:.2f}%.\".format (baseline[-1]['test_score']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "\n",
      "The mean accuracy of the training set is 79.07%.\n",
      "The mean accuracy of the test set is 50.80%.\n"
     ]
    }
   ],
   "source": [
    "baseline2 = make_data_dict(X,y,random_state=43)\n",
    "\n",
    "X_train2 = baseline2[-1]['X_train']\n",
    "y_train2 = baseline2[-1]['y_train']\n",
    "X_test2 = baseline2[-1]['X_test']\n",
    "y_test2 = baseline2[-1]['y_test']\n",
    "\n",
    "scale2 = StandardScaler()\n",
    "baseline2.append(general_transformer(scale2, X_train2, y_train2, X_test2, y_test2))\n",
    "\n",
    "X_train2 = baseline2[-1]['X_train']\n",
    "y_train2 = baseline2[-1]['y_train']\n",
    "X_test2 = baseline2[-1]['X_test']\n",
    "y_test2 = baseline2[-1]['y_test']\n",
    "\n",
    "LogReg2 =LogisticRegression(n_jobs=-1,verbose =3, penalty='l1')\n",
    "baseline2.append(general_model(LogReg2,X_train2, y_train2, X_test2, y_test2))\n",
    "\n",
    "print \"\\n\"\n",
    "print \"The mean accuracy of the training set is {:.2f}%.\".format (baseline2[-1]['train_score']*100)\n",
    "print \"The mean accuracy of the test set is {:.2f}%.\".format (baseline2[-1]['test_score']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.30168855e-03,   1.64321179e-01,   6.26193677e-02,\n",
       "         -7.86641056e-02,   1.87052855e-01,  -1.91899021e-01,\n",
       "         -1.50245561e-01,   6.57424901e-02,   1.06307933e-01,\n",
       "          0.00000000e+00,  -1.34414093e-01,   1.05004051e-01,\n",
       "          7.38551237e-02,   0.00000000e+00,  -8.53339723e-02,\n",
       "          6.60241586e-02,  -1.46308774e-02,  -5.73393270e-02,\n",
       "          8.36447833e-02,  -7.66348670e-02,  -1.12252437e-01,\n",
       "          1.00191938e-01,   0.00000000e+00,  -2.06173370e-01,\n",
       "          4.13895810e-02,   8.23416988e-02,   1.73111099e-01,\n",
       "          9.45515630e-02,   0.00000000e+00,   3.57863455e-02,\n",
       "          2.50597300e-02,  -9.36908164e-03,   0.00000000e+00,\n",
       "          1.03561401e-01,  -1.36318712e-01,   3.29989563e-02,\n",
       "         -3.12240511e-02,   8.88875700e-03,  -1.30732168e-01,\n",
       "         -6.54031335e-02,   0.00000000e+00,   4.81145129e-02,\n",
       "         -1.19081824e-01,  -7.00469889e-02,   1.40300293e-01,\n",
       "          1.00976840e-02,   2.68510360e-01,   1.09302837e-01,\n",
       "          4.79383603e-01,  -1.43569258e-01,   2.84840602e-02,\n",
       "         -3.84361415e-02,  -1.76034889e-02,   1.83130778e-01,\n",
       "          5.43979902e-02,  -4.67046591e-02,   1.94508263e-01,\n",
       "         -9.11665852e-02,   1.43380642e-01,  -4.38429064e-02,\n",
       "         -1.23884314e-01,   2.27799108e-01,   8.23734330e-02,\n",
       "         -8.55016032e-02,   7.50001067e-01,  -1.58861196e-01,\n",
       "         -4.94663001e-02,   4.77021379e-02,  -8.56170813e-02,\n",
       "         -4.43322140e-02,  -3.49589472e-02,  -5.27982208e-02,\n",
       "          4.83435655e-03,   8.91748430e-02,   1.02711727e-01,\n",
       "          1.62821949e-02,   2.51571456e-02,   9.39148517e-02,\n",
       "         -6.86553056e-02,  -6.54672193e-03,  -7.84871519e-02,\n",
       "          4.90036465e-02,  -8.29187248e-02,   1.01468510e-01,\n",
       "         -4.02623762e-02,  -1.39609349e-01,   3.10706003e-02,\n",
       "         -6.01493137e-02,  -1.67459551e-03,  -3.50400283e-02,\n",
       "         -1.28929467e-02,  -1.11334268e-01,   7.93088604e-02,\n",
       "          2.41534260e-02,  -4.64801311e-02,   6.11398196e-03,\n",
       "          3.84307911e-02,   4.66588821e-02,  -1.52215648e-02,\n",
       "          5.18060223e-02,  -3.58535868e-02,   0.00000000e+00,\n",
       "          0.00000000e+00,  -1.88352480e-01,  -1.11871253e-02,\n",
       "          5.93818446e-01,   1.42264422e-01,  -9.88397906e-03,\n",
       "         -6.35026898e-02,  -5.34812752e-02,   4.91472821e-02,\n",
       "          3.77735927e-03,  -9.05598922e-02,   9.16610555e-02,\n",
       "         -4.90713345e-04,  -8.88991572e-02,   6.38521732e-02,\n",
       "          1.58397165e-02,   0.00000000e+00,   9.68916011e-02,\n",
       "          3.65409344e-02,   3.61692369e-02,   4.16367168e-02,\n",
       "          2.14688010e-02,  -9.32495851e-02,  -4.78968642e-02,\n",
       "         -3.85219526e-02,  -6.40507621e-02,   2.82748197e-02,\n",
       "          3.97148705e-02,  -1.34952238e-01,   2.22151935e-02,\n",
       "          2.98967595e-02,   1.15758524e-01,   8.75709714e-02,\n",
       "          1.04083421e-02,   1.24709181e-01,   1.85793250e-01,\n",
       "         -8.05855504e-02,  -2.21376310e-01,   1.34800934e-01,\n",
       "          1.16632249e-01,  -3.10371399e-02,  -2.94002744e-02,\n",
       "          9.58528458e-02,   5.42389215e-02,  -7.33500999e-02,\n",
       "         -1.32522303e-01,  -4.22335119e-02,   9.45654331e-02,\n",
       "         -9.61686075e-02,   2.11739596e-02,  -6.80378189e-02,\n",
       "         -2.11629249e-01,  -5.32068247e-02,  -1.12631293e-01,\n",
       "          2.87908954e-02,  -2.51270672e-03,   1.47431509e-01,\n",
       "         -1.47540283e-01,   0.00000000e+00,  -1.52306234e-01,\n",
       "         -6.64045114e-02,   9.35575978e-02,   6.93256218e-02,\n",
       "          9.27894456e-02,   1.68660694e-02,  -7.13199926e-02,\n",
       "         -1.19908070e-01,   0.00000000e+00,  -6.77287420e-02,\n",
       "         -8.84791006e-02,  -4.74888684e-03,  -1.65692242e-02,\n",
       "         -1.31392905e-01,   0.00000000e+00,   1.02016468e-01,\n",
       "          3.14350004e-02,   0.00000000e+00,  -6.00361958e-02,\n",
       "         -4.86651449e-02,   3.92620672e-02,   1.01390637e-01,\n",
       "         -3.56358462e-02,   1.15453906e-01,   5.13192917e-02,\n",
       "          1.11485549e-01,   2.97118954e-02,   0.00000000e+00,\n",
       "          5.76231609e-02,   1.30285169e-02,   1.07696669e-01,\n",
       "          0.00000000e+00,  -1.30663516e-01,   5.63993372e-03,\n",
       "          1.28306525e-01,   7.80675532e-02,   1.32972710e-01,\n",
       "          1.62157628e-01,  -1.45081503e-01,   2.76044246e-03,\n",
       "          1.28194613e-01,  -1.66401240e-01,   0.00000000e+00,\n",
       "          8.90459309e-02,  -4.75340107e-02,  -6.76678557e-02,\n",
       "          1.53167581e-02,  -5.25128871e-02,   2.22677053e-02,\n",
       "         -5.94011259e-02,   2.37574433e-01,  -8.20349073e-02,\n",
       "          1.82727436e-02,   1.04760871e-01,   1.36675282e-02,\n",
       "         -1.90857597e-01,  -1.70506609e-02,  -3.00365465e-03,\n",
       "         -1.01314533e-01,  -5.95341160e-02,  -1.22290352e-01,\n",
       "          1.36311422e-02,  -4.37692343e-02,  -3.53582802e-02,\n",
       "          1.60558205e-03,  -2.79997328e-01,  -5.27432289e-02,\n",
       "         -8.13048913e-02,  -1.34475988e-02,   7.19554788e-02,\n",
       "          0.00000000e+00,   3.04656102e-02,   3.10594877e-02,\n",
       "          2.56606862e-02,   5.06753426e-02,  -1.36813437e-01,\n",
       "         -1.39749914e-01,   2.68431549e-02,  -9.97359761e-02,\n",
       "         -1.32954542e-02,   0.00000000e+00,   8.25055501e-02,\n",
       "          3.57180436e-02,   4.54668989e-02,   5.45815248e-02,\n",
       "          1.29724006e-02,   1.26453447e-01,   1.66416034e-02,\n",
       "          1.06644603e-01,  -9.59565066e-03,   1.49295325e-01,\n",
       "          5.30331291e-02,  -2.06599936e-01,   4.40256851e-02,\n",
       "         -5.01578285e-02,  -1.48375567e-01,   3.08019563e-02,\n",
       "         -2.84631105e-02,  -6.19267416e-02,  -3.46156626e-04,\n",
       "         -3.76444669e-02,  -3.01201284e-02,  -4.72115613e-02,\n",
       "         -8.99784994e-02,   8.75192670e-02,   4.01538799e-02,\n",
       "         -1.62809362e-02,  -3.22642483e-02,   8.84162459e-02,\n",
       "          1.26080356e-02,   2.78471575e-02,  -5.23565466e-03,\n",
       "         -3.91829942e-02,   1.80651492e-03,   5.35873773e-02,\n",
       "          3.73693878e-02,  -1.96153825e-01,  -2.22196182e-01,\n",
       "          1.27116238e-01,   2.43170863e-03,  -3.36895706e-01,\n",
       "          1.51292536e-01,  -2.56999725e-03,  -2.17698046e-02,\n",
       "         -1.75681669e-01,  -2.53897499e-01,  -9.79680263e-02,\n",
       "         -3.21081473e-02,   1.14769015e-01,   7.11928530e-02,\n",
       "         -2.13089830e-01,  -7.93854547e-02,   8.27555044e-02,\n",
       "          2.70904502e-02,  -3.04511512e-02,  -2.14059025e-01,\n",
       "         -1.56074777e-01,  -1.63958190e-01,   1.66825969e-01,\n",
       "         -1.48499733e-01,  -4.15194082e-03,   9.23294912e-02,\n",
       "         -1.11107003e-01,  -1.73721144e-01,  -1.33080955e-02,\n",
       "         -1.21954253e-01,   7.48557096e-02,   9.12752162e-02,\n",
       "          1.13525046e-01,  -2.39394669e-02,   5.41692869e-02,\n",
       "          0.00000000e+00,  -4.55019078e-02,   2.95337761e-02,\n",
       "         -4.90562651e-02,  -1.76721039e-02,   1.76356212e-01,\n",
       "         -1.99495981e-01,  -2.47996840e-02,   7.30410715e-02,\n",
       "          8.28492521e-02,  -1.94517169e-02,  -1.96673699e-01,\n",
       "         -9.24678425e-02,   2.22965760e-02,  -3.08363785e-02,\n",
       "          0.00000000e+00,   1.60145271e-02,   3.95369368e-02,\n",
       "          1.82471857e-01,  -4.89248106e-02,  -2.95518173e-03,\n",
       "          1.19486818e-01,   1.29066826e-02,  -4.95276274e-02,\n",
       "          0.00000000e+00,  -4.89430654e-02,   0.00000000e+00,\n",
       "          2.52615316e-02,   8.37775414e-02,  -6.62508058e-02,\n",
       "          5.77400006e-02,  -1.38212402e-01,   2.15685578e-01,\n",
       "         -7.90547988e-02,   3.77027556e-02,  -8.75934956e-02,\n",
       "          1.81074688e-01,   6.62589803e-02,   1.50511209e-02,\n",
       "          8.85749992e-02,   1.15803617e-01,   0.00000000e+00,\n",
       "          0.00000000e+00,  -1.83444125e-02,   0.00000000e+00,\n",
       "         -8.99836841e-02,   1.29795362e-01,   5.14282451e-02,\n",
       "         -3.30815115e-02,   0.00000000e+00,  -1.16200508e-01,\n",
       "          1.57165611e-01,   9.24686746e-02,   5.36231514e-03,\n",
       "          0.00000000e+00,  -6.52913378e-02,  -5.49892182e-02,\n",
       "         -1.07905768e-01,   9.68117922e-02,   3.50365796e-02,\n",
       "          7.14077059e-02,   1.05638032e-01,  -1.06388143e-01,\n",
       "          1.80288718e-02,   4.87647925e-03,  -1.95427714e-01,\n",
       "         -6.06905777e-01,  -6.78441510e-02,   8.62796252e-02,\n",
       "         -7.87685309e-02,   4.79694066e-02,   1.87403026e-01,\n",
       "          1.08793780e-01,  -5.76290561e-02,   0.00000000e+00,\n",
       "          1.05274923e-01,  -3.94842847e-02,  -7.09642722e-03,\n",
       "         -3.42945573e-02,   5.73782122e-03,  -6.77634804e-03,\n",
       "         -5.36804844e-02,  -1.15486703e-01,  -1.39549297e-02,\n",
       "          0.00000000e+00,  -3.84773985e-02,  -3.62374255e-02,\n",
       "          1.28197643e-01,   7.38249743e-02,  -1.27426414e-01,\n",
       "          6.68193414e-02,   1.90682268e-01,  -3.58109007e-02,\n",
       "          9.59725163e-02,   4.61116236e-02,  -6.47097533e-02,\n",
       "          1.59142905e-01,  -6.57327582e-02,  -1.00368580e-01,\n",
       "         -1.33270112e-01,   9.39452300e-02,  -1.60697275e-01,\n",
       "         -3.30463343e-02,  -9.57213378e-02,  -1.47862715e-02,\n",
       "         -1.66311500e-01,  -5.31915377e-02,   1.06754870e-01,\n",
       "         -1.44722597e-01,  -7.81068013e-02,  -1.23821117e-01,\n",
       "         -8.97839515e-02,   1.79323923e-01,  -1.67752511e-01,\n",
       "         -3.58368251e-02,   1.11037214e-01,   4.85936981e-02,\n",
       "         -1.27709291e-01,  -7.55138104e-02,   1.97784043e-01,\n",
       "          1.16653131e-01,   0.00000000e+00,  -3.77362460e-02,\n",
       "         -2.71647984e-02,   1.14648698e-01,  -8.82542664e-02,\n",
       "          1.25710921e-02,  -1.38387119e-02,  -1.86458518e-02,\n",
       "         -1.61759464e-01,  -9.68287146e-02,   6.46227113e-02,\n",
       "         -1.89797497e-02,   1.83351337e-01,   1.11487977e-02,\n",
       "         -6.82803123e-02,  -8.46887359e-02,  -1.24865176e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   4.32869718e-02,\n",
       "          2.47762284e-01,   1.52667162e-01,   0.00000000e+00,\n",
       "          7.84388499e-02,   1.26106447e-01,   7.86478898e-02,\n",
       "          8.88201860e-03,  -5.50770367e-03,   3.93858524e-02,\n",
       "         -1.51318110e-01,   1.07736889e-02,  -7.23874306e-02,\n",
       "          0.00000000e+00,   5.23276683e-02,  -1.92985092e-02,\n",
       "          4.05205379e-02,  -1.08344208e-02,  -1.34428748e-02,\n",
       "          1.40564439e-01,   0.00000000e+00,  -1.57486865e-02,\n",
       "          4.38093818e-02,   1.05371678e+00,   4.84175320e-02,\n",
       "         -5.11295684e-05,   7.92818405e-02,  -6.54210529e-02,\n",
       "         -8.19531915e-02,  -1.87145959e-01,   2.87999363e-04,\n",
       "         -1.22021633e-02,   5.86108345e-02,  -6.84117253e-02,\n",
       "         -6.88237919e-02,   9.14827345e-03,   1.11262403e-01,\n",
       "          0.00000000e+00,   1.06979712e-01,  -5.25286016e-02,\n",
       "         -5.93436353e-02,   0.00000000e+00,   7.87237658e-02,\n",
       "         -7.77092605e-02,  -1.81670132e-01,   2.80019652e-02,\n",
       "          5.90961035e-02,   1.20726796e-02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline2[-1]['model'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion.\n",
    "\n",
    "Adding the 'l1' peanalty did not improve the accuracy.  The accuracy of both train and test sets are equivalent to baseline.  Different methods of feature selection will be required.  \n",
    "\n",
    "Very few features were eliminated with this technique.  Since feature elimination is one of teh primary benefits of using the 'l1' penalty, other steps will need to be used to select the salient features.  \n",
    "\n",
    "As an aside, the affect of using the scalar has not been considered.  Below, I'll do a run without appliying the scalar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "\n",
      "The mean accuracy of the training set is 79.07%.\n",
      "The mean accuracy of the test set is 50.00%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda/lib/python2.7/site-packages/sklearn/svm/base.py:920: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "baseline3 = make_data_dict(X,y,random_state=43)\n",
    "\n",
    "X_train3 = baseline3[-1]['X_train']\n",
    "y_train3 = baseline3[-1]['y_train']\n",
    "X_test3 = baseline3[-1]['X_test']\n",
    "y_test3 = baseline3[-1]['y_test']\n",
    "\n",
    "LogReg3  = LogisticRegression(n_jobs=-1,verbose =3, penalty='l1')\n",
    "baseline3.append(general_model(LogReg3,X_train3, y_train3, X_test3, y_test3))\n",
    "\n",
    "print \"\\n\"\n",
    "print \"The mean accuracy of the training set is {:.2f}%.\".format (baseline3[-1]['train_score']*100)\n",
    "print \"The mean accuracy of the test set is {:.2f}%.\".format (baseline3[-1]['test_score']*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion, part 2\n",
    "\n",
    "Including the scalar transformation reduces the computation time compared to doing the regression without scaling.  The results are virtually the same with and without the scalar. However, the scalar helps the anaylsis converge quickly."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
